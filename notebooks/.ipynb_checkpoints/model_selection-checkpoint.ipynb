{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training and Selection\n",
    "---\n",
    "Training and evaluati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/global/home/users/alexandregeorges/Chapter1/')\n",
    "\n",
    "from codebase.params import *\n",
    "from codebase.utils import *\n",
    "\n",
    "import os, pickle, itertools, glob, re, datetime, copy\n",
    "\n",
    "from joblib import dump, load\n",
    "\n",
    "import cv2 as cv\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import rioxarray as rxr\n",
    "import earthpy.plot as ep\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib import colors as colors_mat\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from pyproj import CRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import  HistGradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paths_to_datetimeindex(list):\n",
    "    pattern = PATTERN_REGEX\n",
    "    new_list = []\n",
    "    for item in list:\n",
    "        time = re.search(pattern, item).group(1)\n",
    "        time = datetime.strptime(time, '%m-%d-%Y').date()\n",
    "        new_list.append(time)\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading observations from each into xarray dataset and dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_codes = ['CCHT', 'GPHT', 'CRTT'] #CCHT, CRTT, GPHT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'datetime' has no attribute 'strptime'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m site_files \u001b[38;5;241m=\u001b[39m DOWNLOAD_DIR_ROOT \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTRAINING/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m code \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*.tif\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Getting list of available observation dates from desired sites\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m site_dates \u001b[38;5;241m=\u001b[39m [i\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m paths_to_datetimeindex(glob\u001b[38;5;241m.\u001b[39mglob(site_files))]\n\u001b[1;32m      6\u001b[0m site_dates_xr \u001b[38;5;241m=\u001b[39m xr\u001b[38;5;241m.\u001b[39mVariable(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mObservation Date\u001b[39m\u001b[38;5;124m'\u001b[39m, site_dates)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Putting observations in xarray\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m, in \u001b[0;36mpaths_to_datetimeindex\u001b[0;34m(list)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m      5\u001b[0m     time \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msearch(pattern, item)\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m     time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mstrptime(time, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mdate()\n\u001b[1;32m      7\u001b[0m     new_list\u001b[38;5;241m.\u001b[39mappend(time)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_list\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'datetime' has no attribute 'strptime'"
     ]
    }
   ],
   "source": [
    "sites_data = {}\n",
    "for code in site_codes:\n",
    "    site_files = DOWNLOAD_DIR_ROOT + 'TRAINING/' + code + '/' + '*.tif'\n",
    "    # Getting list of available observation dates from desired sites\n",
    "    site_dates = [i.strftime('%m-%d-%Y') for i in paths_to_datetimeindex(glob.glob(site_files))]\n",
    "    site_dates_xr = xr.Variable('Observation Date', site_dates)\n",
    "    # Putting observations in xarray\n",
    "    site_obs = [rxr.open_rasterio(entry).squeeze() for entry in glob.glob(site_files)]\n",
    "    site_ds = xr.concat(site_obs, dim=site_dates_xr).to_dataset(dim='Observation Date')\n",
    "    # Getting Training Labels from desired sites\n",
    "    labels_dir = 'datasets/Shapefiles/' + code + '_training.shp'\n",
    "    site_labels = gpd.read_file('../' + labels_dir)\n",
    "    # Dropping any possible NaNs from labels\n",
    "    site_labels = site_labels.dropna(axis=0, how='any')\n",
    "\n",
    "    sites_data[code] = {'dataset': site_ds, 'dates': site_dates, 'labels': site_labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sites_data['CCHT']['dataset']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccht = sites_data['CCHT']\n",
    "initial_train = ccht['dataset'][ccht['dates'][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "band_titles = ['Blue', 'Green', 'Red', 'RedEdge', 'NIR', 'NDWI', 'NDVI']\n",
    "ep.plot_bands(initial_train, cols=4);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing and Formating Training Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_crs = CRS.from_epsg(32618)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for code in site_codes:\n",
    "    training_labels = sites_data[code]['labels']\n",
    "    training_labels['Class'].replace('Cropland', 0, inplace=True)\n",
    "    training_labels['Class'].replace('Mangrove', 1, inplace=True)\n",
    "    training_labels['Class'].replace('Mudflat', 2, inplace=True)\n",
    "    training_labels['Class'].replace('Open_Water', 3, inplace=True)\n",
    "    training_labels['Class'].replace('Open_water_and_Intertidal_zone', 3, inplace=True)\n",
    "    training_labels['Class'].replace('Open_Water_and_Mudflat_and_Intertidal_zone', 3, inplace=True)\n",
    "    training_labels['Class'].replace('Other_Vegetation', 0, inplace=True)\n",
    "    training_labels['Class'].replace('Urban', 4, inplace=True)\n",
    "    training_labels.rename(columns={'Class' : 'id'}, inplace=True)\n",
    "    sites_data[code]['labels'] = sites_data[code]['labels'].to_crs(new_crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Masking Water and Urban areas using Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for code in site_codes:\n",
    "    site_ds_masked = copy.deepcopy(sites_data[code]['dataset'])\n",
    "    for time in sites_data[code]['dates']:\n",
    "        obs = site_ds_masked[time]\n",
    "        ndvi_copy = obs[-1].copy()\n",
    "        ndwi_copy = obs[-2].copy()\n",
    "        for i,band in enumerate(obs):\n",
    "            # Masking open water out of site\n",
    "            new_band = np.ma.masked_where(ndwi_copy >= 0.2, band)\n",
    "            # Masking non-vegetation out of site\n",
    "            new_band = np.ma.masked_where(ndvi_copy <= 0.2, new_band)\n",
    "            site_ds_masked[time][i] = new_band\n",
    "    sites_data[code]['dataset'] = site_ds_masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vis = sites_data['CCHT']['dataset'][sites_data['CCHT']['dates'][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "band_titles = ['Blue', 'Green', 'Red', 'RedEdge', 'NIR', 'NDWI', 'NDVI']\n",
    "ep.plot_bands(test_vis, cols=4, title=band_titles);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=500)\n",
    "plt.imshow(test_vis[-1])\n",
    "colorbar = plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification and Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raster Samples and Label cleanup    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_clean(dataset, labels, times):\n",
    "    samples_stack = labels.apply(lambda x: dataset[times[0]].rio.clip([x['geometry']], from_disk=True).squeeze().values, axis=1)\n",
    "    labels['samples'] = samples_stack\n",
    "    n = copy.deepcopy(labels)\n",
    "    \n",
    "    for time in times[1:]:\n",
    "        print(f\"Grabbing pixel values for labels at {time} observation.\")\n",
    "        obs = dataset[time]\n",
    "        \n",
    "        # Matching Label Points with Raster Pixels\n",
    "        labels_n = labels.apply(lambda x: obs.rio.clip([x['geometry']], from_disk=True).squeeze().values, axis=1)\n",
    "        # Samples for pixels changes every loop between observations while labelled point positions stay the same\n",
    "        n['samples'] = labels_n\n",
    "        labels = pd.concat([labels, n], ignore_index=True, axis=0)\n",
    "    \n",
    "    labels['samples'] = labels['samples'].apply(lambda x: x.reshape((1, -1)))\n",
    "    # Finding out which data points exhibit a masked out feature (open water, urban, clouds)\n",
    "    masked_points = labels['samples'].apply(lambda x: any(np.isnan(x[0])))\n",
    "\n",
    "    # Replacing label in data points with NaNs with a new value (marked as their own 'unused' category)\n",
    "    labels.loc[masked_points, 'id'] = 5\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sites_data['CCHT']['dataset'][sites_data['CCHT']['dates'][0]][-2].plot(ax=ax)\n",
    "sites_data['CCHT']['labels'].plot(ax=ax, color='tab:orange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for code in site_codes:\n",
    "    print('Getting samples for '+code)\n",
    "    sites_data[code]['labels'] = label_clean(sites_data[code]['dataset'], sites_data[code]['labels'], sites_data[code]['dates'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_points(dataset, training_labels, times):\n",
    "    X_data_train = []\n",
    "    X_data_test = []\n",
    "    labels_train = np.array([])\n",
    "    labels_test = np.array([])\n",
    "\n",
    "    # Train-Test Split\n",
    "    train_set, test_set = train_test_split(training_labels, test_size=0.3, stratify=training_labels['id'])\n",
    "    # Reshape data \n",
    "    new_train = []\n",
    "    new_test = []\n",
    "    temp_data_train = (train_set['samples']).apply(lambda x: x.reshape((1, -1)))\n",
    "    temp_data_test = (test_set['samples']).apply(lambda x: x.reshape((1, -1)))\n",
    "    for arr in temp_data_train.values:\n",
    "        new_train.append(arr[0])\n",
    "    for arr in temp_data_test.values:\n",
    "        new_test.append(arr[0])    \n",
    "    X_data_train = X_data_train + new_train\n",
    "    X_data_test = X_data_test + new_test\n",
    "        #X_data = X_data + new\n",
    "        \n",
    "        # getting labels repeated for every observation as they are unchanging with position\n",
    "    labels_train = np.append(labels_train, train_set['id'].values)\n",
    "    labels_test = np.append(labels_test, test_set['id'].values)\n",
    "\n",
    "    # Putting train and test arrays together\n",
    "    X_train = np.array(X_data_train)\n",
    "    X_test = np.array(X_data_test)\n",
    "\n",
    "    y_train = np.array(labels_train)\n",
    "    y_test = np.array(labels_test)\n",
    "\n",
    "    # Split data in train and test\n",
    "    #X_data = np.array(X_data)\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(X_data, labels, test_size=0.3, stratify=labels)\n",
    "    print(f\"X_train Shape: {X_train.shape}\\nX_test Shape: {X_test.shape} \\ny_train Shape: {y_train.shape}\\ny_test Shape:{y_test.shape}\")\n",
    "    return X_train, X_test, y_train, y_test, train_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"datasets = sites_data['CCHT']['dataset']\n",
    "training_labels = sites_data['CCHT']['labels']\n",
    "times = sites_data['CCHT']['dates']\n",
    "\n",
    "X_data_train = []\n",
    "X_data_test = []\n",
    "labels_train = np.array([])\n",
    "labels_test = np.array([])\n",
    "\n",
    "    # Train-Test Split\n",
    "train_set, test_set = train_test_split(training_labels, test_size=0.3, stratify=training_labels['id'])\n",
    "train_set['samples'][0]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([])\n",
    "X_test = np.array([])\n",
    "y_train = np.array([])\n",
    "y_test = np.array([])\n",
    "test = []\n",
    "\n",
    "X_train, X_test, y_train, y_test, train_set, test_set = get_train_test_points(sites_data[code]['dataset'], sites_data[code]['labels'], sites_data[code]['dates'])\n",
    "for code in site_codes:\n",
    "    ### CONCATENATE INSTEAD OF APPEND\n",
    "    xtrain, xtest, ytrain, ytest, train_set, test_set = get_train_test_points(sites_data[code]['dataset'], sites_data[code]['labels'], sites_data[code]['dates'])\n",
    "    X_train = np.concatenate((X_train, xtrain))\n",
    "    X_test = np.concatenate((X_test, xtest))\n",
    "    y_train = np.concatenate((y_train, ytrain))\n",
    "    y_test = np.concatenate((y_test, ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacking Image Data to be Trained On"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes = sites_data['CCHT']['dataset'][sites_data['CCHT']['dates'][0]].shape\n",
    "acq = [band.values.reshape(((band.shape)[0])*((band.shape)[1]), 1) for band in sites_data['CCHT']['dataset'][sites_data['CCHT']['dates'][0]]]\n",
    "resh = np.array(acq).reshape(shapes[0], shapes[1]*shapes[2]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['id'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'max_depth': [None, 10],\n",
    "    'max_leaf_nodes': [15, 31],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'n_iter_no_change': [10, 20],\n",
    "    'min_samples_leaf': [1, 10],\n",
    "    'l2_regularization': [0, 1e-3, 1e-6],\n",
    "    'class_weight': [{2:1.25, 1:0.25, 4:10}]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentinel values in RF for NaNs\n",
    "X_encoded = np.nan_to_num(X_train, nan=-99999) # Replace NaNs with a sentinel value\n",
    "X_test_enc = np.nan_to_num(X_test, nan=-99999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgb_classifier = HistGradientBoostingClassifier(random_state=42)\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "grid_search = RandomizedSearchCV(\n",
    "    estimator=hgb_classifier,\n",
    "    param_distributions=param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=5,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "grid_search.fit(X_encoded, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Accuracy:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = grid_search.best_estimator_\n",
    "test_accuracy = best_model.score(X_test_enc, y_test)\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction and Visualization of Whole Site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resh = np.nan_to_num(resh, nan=-99999)\n",
    "hgb_pred = best_model.predict(resh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgb_classified = hgb_pred.reshape(shapes[1], shapes[2])\n",
    "hgb_classified = cv.GaussianBlur(hgb_classified.astype(np.uint8), (3,3), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_cmap = colors_mat.ListedColormap(colors=['mediumseagreen', 'forestgreen', 'tan', 'lightsteelblue', 'gray', 'white'])\n",
    "boundaries = [0, 1, 2, 3, 4, 5]\n",
    "custom_norm = colors_mat.BoundaryNorm(boundaries, custom_cmap.N, clip=True)\n",
    "\n",
    "patches = [mpatches.Patch(color='gray', label='Urban'),\n",
    "           mpatches.Patch(color='mediumseagreen', label='Other Vegetation'),\n",
    "           mpatches.Patch(color='forestgreen', label='Mangrove'),\n",
    "           mpatches.Patch(color='tan', label='Mud Flat'),\n",
    "           mpatches.Patch(color='lightsteelblue', label='Water, Intertidal Zone, and Mudflats'),\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=500)\n",
    "ax.imshow(hgb_classified, cmap=custom_cmap)\n",
    "fig.legend(handles=patches, fancybox=False, bbox_to_anchor=(0.75,0.95), ncol=2)\n",
    "ax.set_title('Land Cover Classification Using Histogram Gradient Boosting')\n",
    "#fig.patch.set_facecolor('xkcd:white')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dumping Model for Classification Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(best_model, '../'+CLASSIFIER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MangroveRS",
   "language": "python",
   "name": "mangrovers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
